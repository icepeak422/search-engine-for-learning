Review : (Function) In this section we’re going to make sure that you’re familiar with functions and function notation.  Both will appear in almost every section in a Calculus class and so you will need to be able to deal with them.First, what exactly is a function?  An equation will be a function if for any x in the domain of the equation (the domain is all the x’s that can be plugged into the equation) the equation will yield exactly one value of y.This is usually easier to understand with an example.Example 1  Determine if each of the following are functions.This first one is a function.  Given an x, there is only one way to square it and then add 1 to the result. So, no matter what value of x you put into the equation, there is only one possible value of y.The only difference between this equation and the first is that we moved the exponent off the x and onto the y.  This small change is all that is required, in this case, to change the equation from a function to something that isn’t a function. To see that this isn’t a function is fairly simple.  Choose a value of x, say x=3 and plug this into the equation.Now, there are two possible values of y that we could use here.  We could use  or .  Since there are two possible values of y that we get from a single x this equation isn’t a function.Note that this only needs to be the case for a single value of x to make an equation not be a function.  For instance we could have used x=-1 and in this case we would get a single y (y=0).  However, because of what happens at x=3 this equation will not be a function. Next we need to take a quick look at function notation.  Function notation is nothing more than a fancy way of writing the y in a function that will allow us to simplify notation and some of our work a little. Let’s take a look at the following function. Using function notation we can write this as any of the following. Recall that this is NOT a letter times x, this is just a fancy way of writing y. So, why is this useful?  Well let’s take the function above and let’s get the value of the function at x=-3.  Using function notation we represent the value of the function at x=-3 as f(-3).  Function notation gives us a nice compact way of representing function values. Now, how do we actually evaluate the function?  That’s really simple.  Everywhere we see an x on the right side we will substitute whatever is in the parenthesis on the left side.  For our function this gives, Let’s take a look at some more function evaluation. Example 2  Given   find each of the following. (a)    [Solution] (b)    [Solution] (c)    [Solution] (d)    [Solution] (e)    [Solution] (f)    [Solution] Solution (a) [Return to Problems] (b) Be careful when squaring negative numbers! [Return to Problems] (c) Remember that we substitute for the x’s WHATEVER is in the parenthesis on the left.  Often this will be something other than a number.  So, in this case we put t’s in for all the x’s on the left. [Return to Problems] (d) Often instead of evaluating functions at numbers or single letters we will have some fairly complex evaluations so make sure that you can do these kinds of evaluations. [Return to Problems] (e) The only difference between this one and the previous one is that I changed the t to an x.  Other than that there is absolutely no difference between the two!  Don’t get excited if an x appears inside the parenthesis on the left. [Return to Problems] (f) This one is not much different from the previous part.  All we did was change the equation that we were plugging into the function. [Return to Problems] All throughout a calculus course we will be finding roots of functions.  A root of a function is nothing more than a number for which the function is zero.  In other words, finding the roots of a function, g(x), is equivalent to solving Example 3  Determine all the roots of Solution So we will need to solve, First, we should factor the equation as much as possible.  Doing this gives, Next recall that if a product of two things are zero then one (or both) of them had to be zero.  This means that, From the first it’s clear that one of the roots must then be t=0.  To get the remaining roots we will need to use the quadratic formula on the second equation.  Doing this gives, In order to remind you how to simplify radicals we gave several forms of the answer. To complete the problem, here is a complete list of all the roots of this function. Note we didn’t use the final form for the roots from the quadratic.  This is usually where we’ll stop with the simplification for these kinds of roots.  Also note that, for the sake of the practice, we broke up the compact form for the two roots of the quadratic.  You will need to be able to do this so make sure that you can. This example had a couple of points other than finding roots of functions. The first was to remind you of the quadratic formula.  This won’t be the last time that you’ll need it in this class. The second was to get you used to seeing “messy” answers.  In fact, the answers in the above list are not that messy.  However, most students come out of an Algebra class very used to seeing only integers and the occasional “nice” fraction as answers. So, here is fair warning.  In this class I often will intentionally make the answers look “messy” just to get you out of the habit of always expecting “nice” answers.  In “real life” (whatever that is) the answer is rarely a simple integer such as two.  In most problems the answer will be a decimal that came about from a messy fraction and/or an answer that involved radicals. One of the more important ideas about functions is that of the domain and range of a function.  In simplest terms the domain of a function is the set of all values that can be plugged into a function and have the function exist and have a real number for a value.  So, for the domain we need to avoid division by zero, square roots of negative numbers, logarithms of zero and logarithms of negative numbers (if not familiar with logarithms we’ll take a look at them a little later), etc.  The range of a function is simply the set of all possible values that a function can take. Let’s find the domain and range of a few functions. Example 4  Find the domain and range of each of the following functions. (a)    [Solution] (b)    [Solution] (c)    [Solution] (d)    [Solution] (e)    [Solution] Solution (a) We know that this is a line and that it’s not a horizontal line (because the slope is 5 and not zero…).  This means that this function can take on any value and so the range is all real numbers.  Using “mathematical” notation this is, This is more generally a polynomial and we know that we can plug any value into a polynomial and so the domain in this case is also all real numbers or, [Return to Problems] (b) This is a square root and we know that square roots are always positive or zero and because we can have the square root of zero in this case, We know then that the range will be, For the domain we have a little bit of work to do, but not much.  We need to make sure that we don’t take square roots of any negative numbers and so we need to require that, The domain is then, [Return to Problems] (c) Here we have a quadratic which is a polynomial and so we again know that the domain is all real numbers or, In this case the range requires a little bit of work.  From an Algebra class we know that the graph of this will be a parabola that opens down (because the coefficient of the  is negative) and so the vertex will be the highest point on the graph.  If we know the vertex we can then get the range.  The vertex is then, So, as discussed, we know that this will be the highest point on the graph or the largest value of the function and the parabola will take all values less than this so the range is then, [Return to Problems] (d) This function contains an absolute value and we know that absolute value will be either positive or zero.  In this case the absolute value will be zero if  and so the absolute value portion of this function will always be greater than or equal to zero.  We are subtracting 3 from the absolute value portion and so we then know that the range will be, We can plug any value into an absolute value and so the domain is once again all real numbers or, [Return to Problems] (e) This function may seem a little tricky at first but is actually the easiest one in this set of examples.  This is a constant function and so an value of x that we plug into the function will yield a value of 8.  This means that the range is a single value or, The domain is all real numbers, [Return to Problems] In general determining the range of a function can be somewhat difficult.  As long as we restrict ourselves down to “simple” functions, some of which we looked at in the previous example, finding the range is not too bad, but for most functions it can be a difficult process. Because of the difficulty in finding the range for a lot of functions we had to keep those in the previous set somewhat simple, which also meant that we couldn’t really look at some of the more complicated domain examples that are liable to be important in a Calculus course.  So, let’s take a look at another set of functions only this time we’ll just look for the domain. Example 5  Find the domain of each of the following functions. (a)    [Solution] (b)    [Solution] (c)    [Solution] Solution (a) Okay, with this problem we need to avoid division by zero and so we need to determine where the denominator is zero which means solving, So, these are the only values of x that we need to avoid and so the domain is, [Return to Problems] (b) In this case we need to avoid square roots of negative numbers and so need to require that, Note that we multiplied the whole inequality by -1 (and remembered to switch the direction of the inequality) to make this easier to deal with.  You’ll need to be able to solve inequalities like this more than a few times in a Calculus course so let’s make sure you can solve these. The first thing that we need to do is determine where the function is zero and that’s not too difficult in this case. So, the function will be zero at  and .    Recall that these points will be the only place where the function may change sign.  It’s not required to change sign at these points, but these will be the only points where the function can change sign.  This means that all we need to do is break up a number line into the three regions that avoid these two points and test the sign of the function at a single point in each of the regions.  If the function is positive at a single point in the region it will be positive at all points in that region because it doesn’t contain the any of the points where the function may change sign.  We’ll have a similar situation if the function is negative for the test point. So, here is a number line showing these computations. Review_Ex4b From this we can see that the only region in which the quadratic (in its modified form) will be negative is in the middle region.  Recalling that we got to the modified region by multiplying the quadratic by a -1 this means that the quadratic under the root will only be positive in the middle region and so the domain for this function is then, [Return to Problems] (c) In this case we have a mixture of the two previous parts.  We have to worry about division by zero and square roots of negative numbers.  We can cover both issues by requiring that, Note that we need the inequality here to be strictly greater than zero to avoid the division by zero issues.  We can either solve this by the method from the previous example or, in this case, it is easy enough to solve by inspection.  The domain is this case is, [Return to Problems] The next topic that we need to discuss here is that of function composition.  The composition of f(x) and g(x) is In other words, compositions are evaluated by plugging the second function listed into the first function listed.  Note as well that order is important here.  Interchanging the order will usually result in a different answer. Example 6  Given   and  find each of the following. (a)    [Solution] (b)    [Solution] (c)    [Solution] (d)    [Solution] Solution (a) In this case we’ve got a number instead of an x but it works in exactly the same way. [Return to Problems] (b) Compare this answer to the next part and notice that answers are NOT the same.  The order in which the functions are listed is important! [Return to Problems] (c) And just to make the point.  This answer is different from the previous part.  Order is important in composition. [Return to Problems] (d) In this case do not get excited about the fact that it’s the same function.  Composition still works the same way. [Return to Problems] Let’s work one more example that will lead us into the next section. Example 7  Given   and  find each of the following. (a) (b) Solution (a) (b) In this case the two compositions were the same and in fact the answer was very simple. This will usually not happen.  However, when the two compositions are the same, or more specifically when the two compositions are both x there is a very nice relationship between the two functions.  We will take a look at that relationship in the next section. Review (Introduction) Previous Section Previous Section	 	Next Section Previous Section Review : Inverse Functions Next Chapter Previous Chapter Limits Calculus I (Notes) / Review / Review : Functions    [Notes] [Practice Problems] [Assignment Problems] [Contact Me] [Downloads] [Privacy Statement] [Site Help] [Site Map] [Terms of Use]      [Switch to Mobile View] © 2003 - 2017 Paul Dawkins
In mathematics (and, in particular, functional analysis) convolution is a mathematical operation on two functions (f and g) to produce a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated[clarification needed]. Convolution is similar to cross-correlation. For discrete real valued signals, they differ only in a time reversal in one of the signals. For continuous signals, the cross-correlation operator is the adjoint operator of the convolution operator. It has applications that include probability, statistics, computer vision, natural language processing, image and signal processing, engineering, and differential equations[citation needed]. The convolution can be defined for functions on groups other than Euclidean space[citation needed]. For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by periodic convolution. (See row 11 at DTFT § Properties.) A discrete convolution can be defined for functions on the set of integers. Generalizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing[citation needed]. Computing the inverse of the convolution operation is known as deconvolution.
In this chapter we will look at solving first order differential equations. The most general first order differential equation can be written as, (1) As we will see in this chapter there is no general formula for the solution to (1). What we will do instead is look at several special cases and see how to solve those. We will also look at some of the theory behind first order differential equations as well as some applications of first order differential equations. Below is a list of the topics discussed in this chapter. Linear Equations  Identifying and solving linear first order differential equations. Separable Equations  Identifying and solving separable first order differential equations.  We’ll also start looking at finding the interval of validity from the solution to a differential equation. Exact Equations  Identifying and solving exact differential equations.  We’ll do a few more interval of validity problems here as well. Bernoulli Differential Equations  In this section we’ll see how to solve the Bernoulli Differential Equation.  This section will also introduce the idea of using a substitution to help us solve differential equations. Substitutions  We’ll pick up where the last section left off and take a look at a couple of other substitutions that can be used to solve some differential equations that we couldn’t otherwise solve. Intervals of Validity  Here we will give an in-depth look at intervals of validity as well as an answer to the existence and uniqueness question for first order differential equations. Modeling with First Order Differential Equations  Using first order differential equations to model physical situations.  The section will show some very real applications of first order differential equations. Equilibrium Solutions  We will look at the behavior of equilibrium solutions and autonomous differential equations. Euler’s Method  In this section we’ll take a brief look at a method for approximating solutions to differential equations.
In this chapter we will be looking at how to use Laplace transforms to solve differential equations.  There are many kinds of transforms out there in the world.  Laplace transforms and Fourier transforms are probably the main two kinds of transforms that are used.  As we will see in later sections we can use Laplace transforms to reduce a differential equation to an algebra problem.  The algebra can be messy on occasion, but it will be simpler than actually solving the differential equation directly in many cases.  Laplace transforms can also be used to solve IVP’s that we can’t use any previous method on. For “simple” differential equations such as those in the first few sections of the last chapter Laplace transforms will be more complicated than we need.  In fact, for most homogeneous differential equations such as those in the last chapter Laplace transforms is significantly longer and not so useful.  Also, many of the “simple” nonhomogeneous differential equations that we saw in the Undetermined Coefficients and Variation of Parameters are still simpler (or at the least no more difficult than Laplace transforms) to do as we did them there.  However, at this point, the amount of work required for Laplace transforms is starting to equal the amount of work we did in those sections. Laplace transforms comes into its own when the forcing function in the differential equation starts getting more complicated.  In the previous chapter we looked only at nonhomogeneous differential equations in which g(t) was a fairly simple continuous function.  In this chapter we will start looking at g(t)’s that are not continuous.  It is these problems where the reasons for using Laplace transforms start to become clear. We will also see that, for some of the more complicated nonhomogeneous differential equations from the last chapter, Laplace transforms are actually easier on those problems as well. Here is a brief rundown of the sections in this chapter. The Definition  The definition of the Laplace transform.  We will also compute a couple Laplace transforms using the definition. Laplace Transforms  As the previous section will demonstrate, computing Laplace transforms directly from the definition can be a fairly painful process.  In this section we introduce the way we usually compute Laplace transforms. Inverse Laplace Transforms  In this section we ask the opposite question.  Here’s a Laplace transform, what function did we originally have? Step Functions  This is one of the more important functions in the use of Laplace transforms.  With the introduction of this function the reason for doing Laplace transforms starts to become apparent. Solving IVP’s with Laplace Transforms  Here’s how we used Laplace transforms to solve IVP’s. Nonconstant Coefficient IVP’s  We will see how Laplace transforms can be used to solve some nonconstant coefficient IVP’s IVP’s with Step Functions  Solving IVP’s that contain step functions.  This is the section where the reason for using Laplace transforms really becomes apparent. Dirac Delta Function  One last function that often shows up in Laplace transform problems. Convolution Integral  A brief introduction to the convolution integral and an application for Laplace transforms. Table of Laplace Transforms  This is a small table of Laplace Transforms that we’ll be using here.
In this chapter we will be looking at how to use Laplace transforms to solve differential equations.  There are many kinds of transforms out there in the world.  Laplace transforms and Fourier transforms are probably the main two kinds of transforms that are used.  As we will see in later sections we can use Laplace transforms to reduce a differential equation to an algebra problem.  The algebra can be messy on occasion, but it will be simpler than actually solving the differential equation directly in many cases.  Laplace transforms can also be used to solve IVP’s that we can’t use any previous method on. For “simple” differential equations such as those in the first few sections of the last chapter Laplace transforms will be more complicated than we need.  In fact, for most homogeneous differential equations such as those in the last chapter Laplace transforms is significantly longer and not so useful.  Also, many of the “simple” nonhomogeneous differential equations that we saw in the Undetermined Coefficients and Variation of Parameters are still simpler (or at the least no more difficult than Laplace transforms) to do as we did them there.  However, at this point, the amount of work required for Laplace transforms is starting to equal the amount of work we did in those sections. Laplace transforms comes into its own when the forcing function in the differential equation starts getting more complicated.  In the previous chapter we looked only at nonhomogeneous differential equations in which g(t) was a fairly simple continuous function.  In this chapter we will start looking at g(t)’s that are not continuous.  It is these problems where the reasons for using Laplace transforms start to become clear. We will also see that, for some of the more complicated nonhomogeneous differential equations from the last chapter, Laplace transforms are actually easier on those problems as well. Here is a brief rundown of the sections in this chapter. The Definition  The definition of the Laplace transform.  We will also compute a couple Laplace transforms using the definition. Laplace Transforms  As the previous section will demonstrate, computing Laplace transforms directly from the definition can be a fairly painful process.  In this section we introduce the way we usually compute Laplace transforms. Inverse Laplace Transforms  In this section we ask the opposite question.  Here’s a Laplace transform, what function did we originally have? Step Functions  This is one of the more important functions in the use of Laplace transforms.  With the introduction of this function the reason for doing Laplace transforms starts to become apparent. Solving IVP’s with Laplace Transforms  Here’s how we used Laplace transforms to solve IVP’s. Nonconstant Coefficient IVP’s  We will see how Laplace transforms can be used to solve some nonconstant coefficient IVP’s IVP’s with Step Functions  Solving IVP’s that contain step functions.  This is the section where the reason for using Laplace transforms really becomes apparent. Dirac Delta Function  One last function that often shows up in Laplace transform problems. Convolution Integral  A brief introduction to the convolution integral and an application for Laplace transforms. Table of Laplace Transforms  This is a small table of Laplace Transforms that we’ll be using here.
In this chapter we’ll be taking a quick and very brief look at a couple of topics.  The two main topics in this chapter are Boundary Value Problems and Fourier Series.  We’ll also take a look at a couple of other topics in this chapter.  The main point of this chapter is to get some of the basics out of the way that we’ll need in the next chapter where we’ll take a look at one of the more common solution methods for partial differential equations. It should be pointed out that both of these topics are far more in depth than what we’ll be covering here.  In fact you can do whole courses on each of these topics.  What we’ll be covering here are simply the basics of these topics that well need in order to do the work in the next chapter.  There are whole areas of both of these topics that we’ll not be even touching on. Here is a brief listing of the topics in this chapter. Boundary Value Problems  In this section we’ll define the boundary value problems as well as work some basic examples. Eigenvalues and Eigenfunctions  Here we’ll take a look at the eigenvalues and eigenfunctions for boundary value problems. Periodic Functions and Orthogonal Functions  We’ll take a look at periodic functions and orthogonal functions in section. Fourier Sine Series  In this section we’ll start looking at Fourier Series by looking at a special case : Fourier Sine Series. Fourier Cosine Series  We’ll continue looking at Fourier Series by taking a look at another special case : Fourier Cosine Series. Fourier Series  Here we will look at the full Fourier series. Convergence of Fourier Series  Here we’ll take a look at some ideas involved in the just what functions the Fourier series converge to as well as differentiation and integration of a Fourier series.
Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. Their essential idea is using randomness to solve problems that might be deterministic in principle. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three distinct problem classes:[1] optimization, numerical integration, and generating draws from a probability distribution. In physics-related problems, Monte Carlo methods are useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean-Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo–based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative "soft" methods.[2] In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is parametrized, mathematicians often use a Markov chain Monte Carlo (MCMC) sampler.[3][4][5][6] The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. That is, in the limit, the samples being generated by the MCMC method will be samples from the desired (target) distribution.[7] By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler. In other problems, the objective is generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depend on the distributions of the current random states (see McKean-Vlasov processes, nonlinear filtering equation).[8][9] In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain.[9][10] A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and MCMC methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.
set of all column vectors (Definition CV) of size m m with entries from the set of complex numbers, C C. When a set similar to this is defined using only column vectors where all the entries are from the real numbers, it is written as R m Rm and is known as Euclidean m m-space. The term vector is used in a variety of different ways. We have defined it as an ordered list written vertically. It could simply be an ordered list of numbers, and perhaps written as ⟨ 2 , 3 , − 1 , 6 ⟩ ⟨2,3,−1,6⟩. Or it could be interpreted as a point in m m dimensions, such as (3 , 4 , − 2 ) (3,4,−2) representing a point in three dimensions relative to x x, y y and z z axes. With an interpretation as a point, we can construct an arrow from the origin to the point which is consistent with the notion that a vector has direction and magnitude. All of these ideas can be shown to be related and equivalent, so keep that in mind as you connect the ideas of this course with ideas from other disciplines. For now, we will stick with the idea that a vector is just a list of numbers, in some particular order. Sage VSCV Vector Spaces of Column Vectors We start our study of this set by first defining what it means for two vectors to be the same. Definition CVE Column Vector Equality Suppose that u , v ∈ C m u,v∈Cm. Then u u and v v are equal, written u = v u=v if [u ] i = [v ] i 1 ≤ i ≤ m [u]i=[v]i1≤i≤m Now this may seem like a silly (or even stupid) thing to say so carefully. Of course two vectors are equal if they are equal for each corresponding entry! Well, this is not as silly as it appears. We will see a few occasions later where the obvious definition is not the right one. And besides, in doing mathematics we need to be very careful about making all the necessary definitions and making them unambiguous. And we have done that here. Notice now that the symbol “=” is now doing triple-duty. We know from our earlier education what it means for two numbers (real or complex) to be equal, and we take this for granted. In Definition SE we defined what it meant for two sets to be equal. Now we have defined what it means for two vectors to be equal, and that definition builds on our definition for when two numbers are equal when we use the condition u i = v i ui=vi for all 1 ≤ i ≤ m 1≤i≤m. So think carefully about your objects when you see an equal sign and think about just which notion of equality you have encountered. This will be especially important when you are asked to construct proofs whose conclusion states that two objects are equal. If you have an electronic copy of the book, such as the PDF version, searching on “Definition CVE” can be an instructive exercise. See how often, and where, the definition is employed. OK, let us do an example of vector equality that begins to hint at the utility of this definition. Example VESE Vector equality for a system of equations We will now define two operations on the set C m Cm. By this we mean well-defined procedures that somehow convert vectors into other vectors. Here are two of the most basic definitions of the entire course. Definition CVA Column Vector Addition Suppose that u , v ∈ C m u,v∈Cm. The sum of u u and v v is the vector u + v u+v defined by [u + v ] i = [u ] i + [v ] i 1 ≤ i ≤ m [u+v]i=[u]i+[v]i1≤i≤m So vector addition takes two vectors of the same size and combines them (in a natural way!) to create a new vector of the same size. Notice that this definition is required, even if we agree that this is the obvious, right, natural or correct way to do it. Notice too that the symbol `+' is being recycled. We all know how to add numbers, but now we have the same symbol extended to double-duty and we use it to indicate how to add two new objects, vectors. And this definition of our new meaning is built on our previous meaning of addition via the expressions u i + v i ui+vi. Think about your objects, especially when doing proofs. Vector addition is easy, here is an example from C 4 C4. Example VA Addition of two vectors in C 4 C4 Our second operation takes two objects of different types, specifically a number and a vector, and combines them to create another vector. In this context we call a number a scalar in order to emphasize that it is not a vector. Definition CVSM Column Vector Scalar Multiplication Suppose u ∈ C m u∈Cm and α ∈ C α∈C, then the scalar multiple of u u by α α is the vector α u αu defined by [α u ] i = α [u ] i 1 ≤ i ≤ m [αu]i=α[u]i1≤i≤m Notice that we are doing a kind of multiplication here, but we are defining a new type, perhaps in what appears to be a natural way. We use juxtaposition (smashing two symbols together side-by-side) to denote this operation rather than using a symbol like we did with vector addition. So this can be another source of confusion. When two symbols are next to each other, are we doing regular old multiplication, the kind we have done for years, or are we doing scalar vector multiplication, the operation we just defined? Think about your objects — if the first object is a scalar, and the second is a vector, then it must be that we are doing our new operation, and the result of this operation will be another vector. Notice how consistency in notation can be an aid here. If we write scalars as lower case Greek letters from the start of the alphabet (such as α α, β β, …) and write vectors in bold Latin letters from the end of the alphabet (u u, v v, …), then we have some hints about what type of objects we are working with. This can be a blessing and a curse, since when we go read another book about linear algebra, or read an application in another discipline (physics, economics, …) the types of notation employed may be very different and hence unfamiliar. Again, computationally, vector scalar multiplication is very easy.
In mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of x and y would be any expression of the form ax + by, where a and b are constants).[1][2][3] The concept of linear combinations is central to linear algebra and related fields of mathematics. Most of this article deals with linear combinations in the context of a vector space over a field, with some generalizations given at the end of the article. Contents  [hide] 1	Definition 2	Examples and counterexamples 2.1	Euclidean vectors 2.2	Functions 2.3	Polynomials 3	The linear span 4	Linear independence 5	Affine, conical, and convex combinations 6	Operad theory 7	Generalizations 8	Application 9	References 10	External links Definition[edit] Suppose that K is a field (for example, the real numbers) and V is a vector space over K. As usual, we call elements of V vectors and call elements of K scalars. If v1,...,vn are vectors and a1,...,an are scalars, then the linear combination of those vectors with those scalars as coefficients is {\displaystyle a_{1}{\vec {v}}_{1}+a_{2}{\vec {v}}_{2}+a_{3}{\vec {v}}_{3}+\cdots +a_{n}{\vec {v}}_{n}.\,} {\displaystyle a_{1}{\vec {v}}_{1}+a_{2}{\vec {v}}_{2}+a_{3}{\vec {v}}_{3}+\cdots +a_{n}{\vec {v}}_{n}.\,} There is some ambiguity in the use of the term "linear combination" as to whether it refers to the expression or to its value. In most cases the value is emphasized, like in the assertion "the set of all linear combinations of v1,...,vn always forms a subspace". However, one could also say "two different linear combinations can have the same value" in which case the expression must have been meant. The subtle difference between these uses is the essence of the notion of linear dependence: a family F of vectors is linearly independent precisely if any linear combination of the vectors in F (as value) is uniquely so (as expression). In any case, even when viewed as expressions, all that matters about a linear combination is the coefficient of each vi; trivial modifications such as permuting the terms or adding terms with zero coefficient do not give distinct linear combinations. In a given situation, K and V may be specified explicitly, or they may be obvious from context. In that case, we often speak of a linear combination of the vectors v1,...,vn, with the coefficients unspecified (except that they must belong to K). Or, if S is a subset of V, we may speak of a linear combination of vectors in S, where both the coefficients and the vectors are unspecified, except that the vectors must belong to the set S (and the coefficients must belong to K). Finally, we may speak simply of a linear combination, where nothing is specified (except that the vectors must belong to V and the coefficients must belong to K); in this case one is probably referring to the expression, since every vector in V is certainly the value of some linear combination. Note that by definition, a linear combination involves only finitely many vectors (except as described in Generalizations below). However, the set S that the vectors are taken from (if one is mentioned) can still be infinite; each individual linear combination will only involve finitely many vectors. Also, there is no reason that n cannot be zero; in that case, we declare by convention that the result of the linear combination is the zero vector in V.
In mathematics, any vector space V has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants. The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the algebraic dual space. When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the continuous dual space. Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis. Contents  [hide] 1	Algebraic dual space 1.1	Finite-dimensional case 1.2	Infinite-dimensional case 1.3	Bilinear products and dual spaces 1.4	Injection into the double-dual 1.5	Transpose of a linear map 1.6	Quotient spaces and annihilators 2	Continuous dual space 2.1	Examples 2.2	Transpose of a continuous linear map 2.3	Annihilators 2.4	Further properties 2.5	Topologies on the dual 2.6	Double dual 3	See also 4	Notes 5	References Algebraic dual space[edit] Given any vector space V over a field F, the (algebraic) dual space V∗ (alternatively denoted by {\displaystyle V^{\vee }} {\displaystyle V^{\vee }} or {\displaystyle V'} V')[1] is defined as the set of all linear maps φ: V → F (linear functionals). Since linear maps are vector space homomorphisms, the dual space is also sometimes denoted by Hom(V, F). The dual space V∗ itself becomes a vector space over F when equipped with an addition and scalar multiplication satisfying: {\displaystyle {\begin{aligned}&(\varphi +\psi )(x)=\varphi (x)+\psi (x)\\&(a\varphi )(x)=a\left(\varphi (x)\right)\end{aligned}}} \begin{align} & (\varphi + \psi)(x) = \varphi(x) + \psi(x) \\ & (a \varphi)(x) = a \left(\varphi(x)\right) \end{align} for all φ and ψ ∈ V∗, x ∈ V, and a ∈ F. Elements of the algebraic dual space V∗ are sometimes called covectors or one-forms. The pairing of a functional φ in the dual space V∗ and an element x of V is sometimes denoted by a bracket: φ(x) = [x,φ] [2] or φ(x) = ⟨φ,x⟩.[3] This pairing defines a nondegenerate bilinear mapping[4] ⟨·,·⟩ : V∗ × V → F called the natural pairing. Finite-dimensional case[edit] If V is finite-dimensional, then V∗ has the same dimension as V. Given a basis {e1, ..., en} in V, it is possible to construct a specific basis in V∗, called the dual basis. This dual basis is a set {e1, ..., en} of linear functionals on V, defined by the relation {\displaystyle \mathbf {e} ^{i}(c^{1}\mathbf {e} _{1}+\cdots +c^{n}\mathbf {e} _{n})=c^{i},\quad i=1,\ldots ,n}  \mathbf{e}^i(c^1 \mathbf{e}_1+\cdots+c^n\mathbf{e}_n) = c^i, \quad i=1,\ldots,n for any choice of coefficients ci ∈ F. In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equations {\displaystyle \mathbf {e} ^{i}(\mathbf {e} _{j})=\delta _{j}^{i}}  \mathbf{e}^i(\mathbf{e}_j) = \delta^{i}_{j} where {\displaystyle \delta _{j}^{i}} \delta^{i}_{j} is the Kronecker delta symbol. For example, if V is R2, and its basis chosen to be {e1 = (1, 0), e2 = (0, 1)}, then e1 and e2 are one-forms (functions that map a vector to a scalar) such that e1(e1) = 1, e1(e2) = 0, e2(e1) = 0, and e2(e2) = 1. (Note: The superscript here is the index, not an exponent). In particular, if we interpret Rn as the space of columns of n real numbers, its dual space is typically written as the space of rows of n real numbers. Such a row acts on Rn as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a real number y. Then, seeing this functional as a matrix M, and x, y as a n × 1 matrix and a 1 × 1 matrix (trivially, a real number) respectively, if we have Mx = y, then, by dimension reasons, M must be a 1 × n matrix, i.e., M must be a row vector. If V consists of the space of geometrical vectors in the plane, then the level curves of an element of V∗ form a family of parallel lines in V, because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of V∗ can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if V is a vector space of any dimension, then the level sets of a linear functional in V∗ are parallel hyperplanes in V, and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.[5] Infinite-dimensional case[edit] If V is not finite-dimensional but has a basis[6] eα indexed by an infinite set A, then the same construction as in the finite-dimensional case yields linearly independent elements eα (α ∈ A) of the dual space, but they will not form a basis. Consider, for instance, the space R∞, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for i ∈ N, ei is the sequence consisting of all zeroes except in the ith position, which is 1. The dual space of R∞ is (isomorphic to) RN, the space of all sequences of real numbers: such a sequence (an) is applied to an element (xn) of R∞ to give the number ∑anxn, which is a finite sum because there are only finitely many nonzero xn. The dimension of R∞ is countably infinite, whereas RN does not have a countable basis. This observation generalizes to any[6] infinite-dimensional vector space V over any field F: a choice of basis {eα : α ∈ A} identifies V with the space (FA)0 of functions f : A → F such that fα = f(α) is nonzero for only finitely many α ∈ A, where such a function f is identified with the vector {\displaystyle \sum _{\alpha \in A}f_{\alpha }\mathbf {e} _{\alpha }} \sum_{\alpha\in A} f_\alpha\mathbf{e}_\alpha in V (the sum is finite by the assumption on f, and any v ∈ V may be written in this way by the definition of the basis). The dual space of V may then be identified with the space FA of all functions from A to F: a linear functional T on V is uniquely determined by the values θα = T(eα) it takes on the basis of V, and any function θ : A → F (with θ(α) = θα) defines a linear functional T on V by {\displaystyle T{\biggl (}\sum _{\alpha \in A}f_{\alpha }\mathbf {e} _{\alpha }{\biggr )}=\sum _{\alpha \in A}f_{\alpha }T(e_{\alpha })=\sum _{\alpha \in A}f_{\alpha }\theta _{\alpha }.} T\biggl(\sum_{\alpha\in A} f_\alpha \mathbf{e}_\alpha\biggr) = \sum_{\alpha \in A} f_\alpha T(e_\alpha) = \sum_{\alpha\in A} f_\alpha \theta_\alpha. Again the sum is finite because fα is nonzero for only finitely many α. Note that (FA)0 may be identified (essentially by definition) with the direct sum of infinitely many copies of F (viewed as a 1-dimensional vector space over itself) indexed by A, i.e., there are linear isomorphisms {\displaystyle V\cong (F^{A})_{0}\cong \bigoplus _{\alpha \in A}{F}.} V\cong (F^A)_0\cong\bigoplus_{\alpha\in A} {F}. On the other hand, FA is (again by definition), the direct product of infinitely many copies of F indexed by A, and so the identification {\displaystyle V^{*}\cong {\biggl (}\bigoplus _{\alpha \in A}F{\biggr )}^{*}\cong \prod _{\alpha \in A}F^{*}\cong \prod _{\alpha \in A}F\cong F^{A}} V^* \cong \biggl(\bigoplus_{\alpha\in A}F\biggr)^* \cong \prod_{\alpha\in A}F^* \cong \prod_{\alpha\in A}F \cong F^A is a special case of a general result relating direct sums (of modules) to direct products. Thus if the basis is infinite, then the algebraic dual space is always of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional. Bilinear products and dual spaces[edit] If V is finite-dimensional, then V is isomorphic to V∗. But there is in general no natural isomorphism between these two spaces.[7] Any bilinear form ⟨·,·⟩ on V gives a mapping of V into its dual space via {\displaystyle v\mapsto \langle v,\cdot \rangle } v\mapsto \langle v, \cdot\rangle where the right hand side is defined as the functional on V taking each w ∈ V to ⟨v,w⟩. In other words, the bilinear form determines a linear mapping {\displaystyle \Phi _{\langle \cdot ,\cdot \rangle }:V\to V^{*}} \Phi_{\langle\cdot,\cdot\rangle} : V\to V^* defined by {\displaystyle [\Phi _{\langle \cdot ,\cdot \rangle }(v),w]=\langle v,w\rangle .} [\Phi_{\langle\cdot,\cdot\rangle}(v),w] = \langle v, w\rangle. If the bilinear form is nondegenerate, then this is an isomorphism onto a subspace of V∗. If V is finite-dimensional, then this is an isomorphism onto all of V∗. Conversely, any isomorphism {\displaystyle \Phi } \Phi  from V to a subspace of V∗ (resp., all of V∗ if V is finite dimensional) defines a unique nondegenerate bilinear form {\displaystyle \langle \cdot ,\cdot \rangle _{\Phi }} {\displaystyle \langle \cdot ,\cdot \rangle _{\Phi }} on V by {\displaystyle \langle v,w\rangle _{\Phi }=(\Phi (v))(w)=[\Phi (v),w].\,}  \langle v,w \rangle_\Phi = (\Phi (v))(w) = [\Phi (v),w].\, Thus there is a one-to-one correspondence between isomorphisms of V to subspaces of (resp., all of) V∗ and nondegenerate bilinear forms on V. If the vector space V is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms. In that case, a given sesquilinear form ⟨·,·⟩ determines an isomorphism of V with the complex conjugate of the dual space {\displaystyle \Phi _{\langle \cdot ,\cdot \rangle }:V\to {\overline {V^{*}}}.} \Phi _{{\langle \cdot ,\cdot \rangle }}:V\to \overline {V^{*}}. The conjugate space V∗ can be identified with the set of all additive complex-valued functionals f: V → C such that {\displaystyle f(\alpha v)={\overline {\alpha }}f(v).}